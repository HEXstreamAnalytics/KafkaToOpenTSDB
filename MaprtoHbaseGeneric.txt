import org.apache.spark._
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._
import org.apache.spark.sql._
import org.apache.spark.rdd.RDD
import com.mapr.db.spark.impl.OJAIDocument
import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType, LongType}
import org.apache.spark.SparkConf
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.kafka.clients.consumer.ConsumerRecord
import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.spark.streaming.kafka09.LocationStrategies.PreferConsistent
import org.apache.spark.streaming.kafka09.ConsumerStrategies.Subscribe
import org.apache.spark.storage.StorageLevel
import org.apache.spark.SparkContext
import kafka.serializer.StringDecoder
import org.apache.spark.streaming.kafka09.{ConsumerStrategies, KafkaUtils, LocationStrategies}
import org.apache.kafka.clients.consumer.ConsumerConfig
import java.util.Properties
import org.apache.spark.sql.SaveMode
import java.time.LocalDateTime
import json
import org.apache.spark.sql.SQLContext.implicits._
import org.apache.spark.sql.Dataset
import org.apache.spark.rdd.NewHadoopRDD
import org.apache.hadoop.hbase.{HBaseConfiguration, HTableDescriptor}
import org.apache.hadoop.hbase.{HConstants, HBaseConfiguration}
import org.apache.hadoop.mapreduce.lib.input.{KeyValueTextInputFormat, TextInputFormat}
import org.apache.hadoop.hbase.client.HBaseAdmin
import org.apache.hadoop.hbase.client.{Put, Result}
import org.apache.hadoop.hbase.client._
import org.apache.hadoop.hbase.mapreduce.TableInputFormat
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.client.HTable;
import org.apache.hadoop.hbase.mapred.TableOutputFormat
import org.apache.hadoop.mapred.JobConf
import org.apache.hadoop.hbase.io.ImmutableBytesWritable
import org.apache.hadoop.mapreduce.Job
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat
import org.apache.hadoop.hbase.KeyValue
import org.apache.hadoop.io.Text
import org.apache.spark.sql._
import org.apache.hadoop.io._
import org.apache.hadoop.hbase.mapreduce.HFileOutputFormat
import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.spark.streaming.kafka09.LocationStrategies.PreferConsistent
import org.apache.spark.streaming.kafka09.ConsumerStrategies.Subscribe
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.kafka09.{ConsumerStrategies, KafkaUtils, LocationStrategies}
import org.apache.kafka.clients.consumer.ConsumerConfig
import org.apache.spark.scheduler.SparkListener
import org.apache.spark.scheduler.SparkListenerStageCompleted
import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.SaveMode
import org.json4s.jackson.JsonMethods.{compact, render}
import scala.util.control.NonFatal
import org.joda.time.DateTime
import org.apache.log4j.Logger
import org.apache.spark.streaming.dstream.DStream
import org.apache.http.client.methods.HttpPost
import org.apache.http.entity.StringEntity
import org.apache.http.impl.client.DefaultHttpClient
import org.json4s._
import org.json4s.jackson.JsonMethods._
import org.json4s.JsonDSL._
import org.apache.hadoop.conf.Configuration

def parseAvroRdd(rdd: RDD[String]): RDD[String] = {
   return rdd.map(jsonString => {jsonString.substring(0,jsonString.length -1).split("\"payload\":")(1)})
   }
object SUPPLIER extends Serializable{
  def main(args: Array[String]) {
 // val sc = new SparkConf().setAppName("test")
      val ssc = new StreamingContext(sc, Seconds(3))
val kafkaParams = Map[String,String](ConsumerConfig.GROUP_ID_CONFIG -> "test5",
ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG -> "172.21.5.112:9092",
ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG ->"org.apache.kafka.common.serialization.StringDeserializer",
ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG ->"org.apache.kafka.common.serialization.StringDeserializer",
  ConsumerConfig.AUTO_OFFSET_RESET_CONFIG -> "latest",
  ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG -> "true"
  )
  //val topicname = args(0)
  //val topic = "topicname"
  //val topic = "/HARMONIAV2:HARMONIA_PRODUCT_DETAILS"
  val topic = args(0)
  val topicSet = topic.split(",").toSet
  val consumerStrategy = ConsumerStrategies.Subscribe[String, String](topicSet, kafkaParams)
  val lines = KafkaUtils.createDirectStream[String, String](ssc, LocationStrategies.PreferConsistent, consumerStrategy)
  val tmp = lines.map(_.value())
  tmp.print()
 tmp.foreachRDD{rdd=>
val x = spark.read.json(parseAvroRdd(rdd));
     val dataframe = x.toDF()
    var number_of_columns = dataframe.columns.length -1
    rdd.foreachPartition(iter => {
    iter.foreach(record => {
        val hConf = HBaseConfiguration.create()
        val tableName = args(1)
        val hTable = new HTable(hConf, tableName)
      // println("entered1")
        for (i <- 0 to number_of_columns){
                val x_old  = record.substring(0, record.length - 1).split("\"payload\":")(1).split(",")(i).split(":")(1)
                val y_old  = record.substring(0, record.length - 1).split("\"payload\":")(1).split(",")(i).split(":")(0).split("}")(0)
                val x = x_old.replaceAll("\\W", "")
                val y = y_old.replaceAll("\\W", "")
                //val keycolumnnumstr = args(2).toInt
                //val keycolumnnum = keycolumnnumstr.toInt
                //val id_con = record.substring(1, record.length - 1).split("\"payload\":")(1).split(",")(keycolumnnum).split(":")(1)
                val timestamp = new DateTime().getMillis / 1000
                //val id_str = timestamp
               // println("entered2")
                val id = tableName+"_"+timestamp
                val thePut = new Put(Bytes.toBytes(id))
                val columnfamily = args(2)
                thePut.add(Bytes.toBytes(columnfamily), Bytes.toBytes(y), Bytes.toBytes(x))
                hTable.put(thePut);
     } })
    })
}
ssc.start()
ssc.awaitTermination()
}
}
